#!/usr/bin/env python3
"""
ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ã„æ–¹:
    python scripts/download_and_backtest.py

æ©Ÿèƒ½:
    1. ç›£è¦–éŠ˜æŸ„ã®æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ1å¹´åˆ†ï¼‰
    2. CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    3. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæ¯”è¼ƒã‚’å®Ÿè¡Œ
    4. çµæœã‚’ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
"""
import sys
import os
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
import time

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from config.settings import DATA_DIR, REPORTS_DIR

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)

# ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
STOCK_DATA_DIR = DATA_DIR / "stock_prices"
STOCK_DATA_DIR.mkdir(parents=True, exist_ok=True)


def get_watchlist() -> List[str]:
    """ç›£è¦–éŠ˜æŸ„ãƒªã‚¹ãƒˆã‚’å–å¾—"""
    watchlist_file = DATA_DIR / "watchlist.json"

    if watchlist_file.exists():
        with open(watchlist_file, 'r') as f:
            data = json.load(f)
            # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”¨ã«ä¸Šä½éŠ˜æŸ„ã®ã¿
            return data.get("symbols", [])[:30]

    return [
        "7203.JP", "6758.JP", "9984.JP", "6861.JP", "8306.JP",
        "9432.JP", "6501.JP", "7267.JP", "8035.JP", "6902.JP",
    ]


def download_stock_data_stooq(symbols: List[str], days: int = 365) -> Dict[str, 'pd.DataFrame']:
    """
    Stooqã‹ã‚‰æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    """
    try:
        import pandas as pd
        from pandas_datareader import data as pdr
    except ImportError:
        logger.error("pandas-datareader ãŒå¿…è¦ã§ã™: pip install pandas-datareader")
        return {}

    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)

    stock_data = {}
    failed = []

    logger.info(f"ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {len(symbols)}éŠ˜æŸ„")
    logger.info(f"   æœŸé–“: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")

    for i, symbol in enumerate(symbols):
        try:
            # Stooqå½¢å¼ã«å¤‰æ›
            stooq_symbol = symbol.split('.')[0] + ".JP"

            logger.info(f"  [{i+1}/{len(symbols)}] {symbol} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")

            df = pdr.DataReader(
                stooq_symbol,
                'stooq',
                start=start_date,
                end=end_date
            )

            if df is not None and len(df) > 60:
                df = df.sort_index()  # Stooqã¯é€†é †ãªã®ã§ã‚½ãƒ¼ãƒˆ
                stock_data[symbol] = df

                # CSVã¨ã—ã¦ä¿å­˜
                save_path = STOCK_DATA_DIR / f"{symbol.replace('.', '_')}.csv"
                df.to_csv(save_path)
                logger.info(f"    âœ“ {len(df)}æ—¥åˆ† â†’ {save_path.name}")
            else:
                failed.append(symbol)
                logger.warning(f"    âœ— ãƒ‡ãƒ¼ã‚¿ä¸è¶³")

            # Rate limiting
            time.sleep(1)

        except Exception as e:
            failed.append(symbol)
            logger.warning(f"    âœ— ã‚¨ãƒ©ãƒ¼: {e}")

    logger.info(f"\nğŸ“Š ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(stock_data)}/{len(symbols)} éŠ˜æŸ„")

    if failed:
        logger.warning(f"   å¤±æ•—: {', '.join(failed[:5])}{'...' if len(failed) > 5 else ''}")

    return stock_data


def load_saved_data() -> Dict[str, 'pd.DataFrame']:
    """ä¿å­˜æ¸ˆã¿ã®CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    import pandas as pd

    stock_data = {}
    csv_files = list(STOCK_DATA_DIR.glob("*.csv"))

    if not csv_files:
        logger.warning("ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return {}

    logger.info(f"ğŸ“‚ ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­: {len(csv_files)}ãƒ•ã‚¡ã‚¤ãƒ«")

    for csv_file in csv_files:
        try:
            symbol = csv_file.stem.replace('_', '.')
            df = pd.read_csv(csv_file, index_col=0, parse_dates=True)

            if len(df) > 60:
                stock_data[symbol] = df
                logger.debug(f"  âœ“ {symbol}: {len(df)}æ—¥åˆ†")
        except Exception as e:
            logger.warning(f"  âœ— {csv_file.name}: {e}")

    logger.info(f"   èª­ã¿è¾¼ã¿å®Œäº†: {len(stock_data)}éŠ˜æŸ„")
    return stock_data


def run_backtest_comparison(stock_data: Dict) -> Dict:
    """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæ¯”è¼ƒã‚’å®Ÿè¡Œ"""
    import pandas as pd
    from backtesting.backtest_engine import BacktestEngine
    from backtesting.enhanced_backtest_engine import EnhancedBacktestEngine

    if len(stock_data) < 3:
        logger.error("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã«ã¯æœ€ä½3éŠ˜æŸ„å¿…è¦ã§ã™")
        return {}

    # æ—¥ä»˜ç¯„å›²ã‚’å–å¾—
    all_dates = set()
    for df in stock_data.values():
        all_dates.update(df.index)
    all_dates = sorted(all_dates)

    start_date = all_dates[60].strftime("%Y-%m-%d")  # æŒ‡æ¨™è¨ˆç®—ã«60æ—¥å¿…è¦
    end_date = all_dates[-1].strftime("%Y-%m-%d")

    logger.info(f"\nğŸ”¬ ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    logger.info(f"   æœŸé–“: {start_date} ~ {end_date}")
    logger.info(f"   éŠ˜æŸ„æ•°: {len(stock_data)}")

    # ã‚»ã‚¯ã‚¿ãƒ¼ãƒãƒƒãƒ—ã‚’èª­ã¿è¾¼ã¿
    sector_map = load_sector_map()

    # åŸºæœ¬ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
    logger.info("\n   [1/2] åŸºæœ¬ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ...")
    basic_engine = BacktestEngine(
        initial_capital=1_000_000,
        max_positions=3,
    )
    basic_results = basic_engine.run_backtest(stock_data, start_date, end_date)

    # æ”¹å–„ç‰ˆãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
    logger.info("   [2/2] æ”¹å–„ç‰ˆãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ...")
    enhanced_engine = EnhancedBacktestEngine(
        initial_capital=1_000_000,
        max_positions=3,
        enable_trailing_stop=True,
        enable_volatility_sizing=True,
        enable_market_regime=True,
        enable_multi_timeframe=True,
        enable_volume_breakout=True,
        enable_swing_low_stop=True,
        enable_additional_filters=True,
        enable_compound=True,
        min_score=55,
        min_confidence=0.60,
    )
    enhanced_results = enhanced_engine.run_backtest(stock_data, start_date, end_date, sector_map)

    return {
        "basic": basic_results,
        "enhanced": enhanced_results,
        "start_date": start_date,
        "end_date": end_date,
    }


def load_sector_map() -> Dict[str, str]:
    """ã‚»ã‚¯ã‚¿ãƒ¼ãƒãƒƒãƒ—ã‚’èª­ã¿è¾¼ã‚€"""
    watchlist_file = DATA_DIR / "watchlist.json"

    if watchlist_file.exists():
        with open(watchlist_file, 'r') as f:
            data = json.load(f)
            sectors = data.get("sectors", {})

            sector_map = {}
            for sector_name, symbols in sectors.items():
                for symbol in symbols:
                    sector_map[symbol] = sector_name
            return sector_map

    return {}


def print_results(results: Dict):
    """çµæœã‚’è¡¨ç¤º"""
    basic = results["basic"]
    enhanced = results["enhanced"]

    print("\n" + "=" * 70)
    print("ğŸ“Š ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœæ¯”è¼ƒ")
    print("=" * 70)
    print(f"æœŸé–“: {results['start_date']} ~ {results['end_date']}")

    print(f"\n{'æŒ‡æ¨™':<20} {'åŸºæœ¬':>15} {'æ”¹å–„ç‰ˆ':>15} {'å¤‰åŒ–':>15}")
    print("-" * 70)

    metrics = [
        ("å–å¼•å›æ•°", basic.num_trades, enhanced.num_trades, ""),
        ("å‹ç‡", basic.win_rate * 100, enhanced.win_rate * 100, "%"),
        ("å¹³å‡å‹ã¡", basic.avg_win, enhanced.avg_win, "%"),
        ("å¹³å‡è² ã‘", basic.avg_loss, enhanced.avg_loss, "%"),
        ("ãƒ—ãƒ­ãƒ•ã‚£ãƒƒãƒˆF", basic.profit_factor, enhanced.profit_factor, ""),
        ("ã‚·ãƒ£ãƒ¼ãƒ—æ¯”", basic.sharpe_ratio, enhanced.sharpe_ratio, ""),
        ("æœ€å¤§DD", basic.max_drawdown, enhanced.max_drawdown, "%"),
        ("ç·ãƒªã‚¿ãƒ¼ãƒ³", basic.total_return_pct, enhanced.total_return_pct, "%"),
        ("æœ€çµ‚è³‡ç”£", basic.final_capital, enhanced.final_capital, "å††"),
    ]

    for name, b_val, e_val, unit in metrics:
        if unit == "å††":
            change = e_val - b_val
            print(f"{name:<20} {b_val:>14,.0f} {e_val:>14,.0f} {change:>+14,.0f}")
        else:
            change = e_val - b_val
            if name == "æœ€å¤§DD":
                indicator = "âœ“" if change < 0 else "âœ—"
            else:
                indicator = "âœ“" if change > 0 else "âœ—"
            print(f"{name:<20} {b_val:>13.2f}{unit:>2} {e_val:>13.2f}{unit:>2} {change:>+13.2f}{unit:>2} {indicator}")

    # ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 70)
    print("ğŸ“ˆ æ”¹å–„åŠ¹æœã‚µãƒãƒªãƒ¼")
    print("=" * 70)

    improvements = [
        ("å‹ç‡", enhanced.win_rate > basic.win_rate,
         f"{(enhanced.win_rate - basic.win_rate) * 100:+.1f}%"),
        ("ãƒªã‚¿ãƒ¼ãƒ³", enhanced.total_return_pct > basic.total_return_pct,
         f"{enhanced.total_return_pct - basic.total_return_pct:+.1f}%"),
        ("ã‚·ãƒ£ãƒ¼ãƒ—æ¯”", enhanced.sharpe_ratio > basic.sharpe_ratio,
         f"{enhanced.sharpe_ratio - basic.sharpe_ratio:+.2f}"),
        ("ãƒªã‚¹ã‚¯(DD)", enhanced.max_drawdown < basic.max_drawdown,
         f"{basic.max_drawdown - enhanced.max_drawdown:+.1f}%"),
    ]

    for name, improved, change in improvements:
        status = "âœ…" if improved else "âŒ"
        print(f"  {name:<15}: {status} ({change})")


def save_results_json(results: Dict):
    """çµæœã‚’JSONã§ä¿å­˜"""
    basic = results["basic"]
    enhanced = results["enhanced"]

    output_file = REPORTS_DIR / f"backtest_results_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
    REPORTS_DIR.mkdir(parents=True, exist_ok=True)

    def serialize(r):
        return {
            "num_trades": r.num_trades,
            "win_rate": float(r.win_rate),
            "avg_win": float(r.avg_win),
            "avg_loss": float(r.avg_loss),
            "profit_factor": float(r.profit_factor),
            "sharpe_ratio": float(r.sharpe_ratio),
            "max_drawdown": float(r.max_drawdown),
            "total_return_pct": float(r.total_return_pct),
            "final_capital": float(r.final_capital),
        }

    output_data = {
        "date": datetime.now().isoformat(),
        "period": {
            "start": results["start_date"],
            "end": results["end_date"],
        },
        "basic": serialize(basic),
        "enhanced": serialize(enhanced),
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ çµæœä¿å­˜: {output_file}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\n" + "=" * 70)
    print("ğŸš€ ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆæ¤œè¨¼")
    print("=" * 70)

    # 1. ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
    stock_data = load_saved_data()

    # 2. ãƒ‡ãƒ¼ã‚¿ãŒãªã‘ã‚Œã°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if len(stock_data) < 5:
        logger.info("\nä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™...")

        try:
            symbols = get_watchlist()
            stock_data = download_stock_data_stooq(symbols, days=365)
        except Exception as e:
            logger.error(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            logger.info("\nğŸ’¡ æ‰‹å‹•ã§ãƒ‡ãƒ¼ã‚¿ã‚’é…ç½®ã™ã‚‹å ´åˆ:")
            logger.info(f"   CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ {STOCK_DATA_DIR} ã«é…ç½®ã—ã¦ãã ã•ã„")
            logger.info("   å½¢å¼: Date,Open,High,Low,Close,Volume")
            return 1

    if len(stock_data) < 3:
        logger.error("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        logger.info(f"\nğŸ’¡ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ {STOCK_DATA_DIR} ã«é…ç½®ã—ã¦ãã ã•ã„")
        return 1

    # 3. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    try:
        results = run_backtest_comparison(stock_data)

        if results:
            # 4. çµæœè¡¨ç¤º
            print_results(results)

            # 5. çµæœä¿å­˜
            save_results_json(results)

            print("\nâœ… æ¤œè¨¼å®Œäº†!")
        else:
            logger.error("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            return 1

    except Exception as e:
        logger.error(f"ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())
